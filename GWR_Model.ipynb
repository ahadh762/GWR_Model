{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # ignore future warning regarding pysal/spaghetti\n",
    "\n",
    "from pysal.lib import weights\n",
    "from pysal.explore import esda\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import contextily\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Rasterio Installation bug where the Environment variable 'PROJ_LIB' isn't looking in the right file for 'Proj.db' SQLlite Database\n",
    "os.environ['PROJ_LIB'] = '/Users/ahadhussain/opt/anaconda3/lib/python3.9/site-packages/rasterio/proj_data/'\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point # Shapely for converting latitude/\n",
    "import pandas as pd\n",
    "import pymssql\n",
    "\n",
    "database = \"Pushing-P-DB\"\n",
    "table = \"dbo.Training_Set\"\n",
    "user = \"pushing_p\"\n",
    "password  = \"t3stP@ssword\"\n",
    "server = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\"\n",
    "\n",
    "\n",
    "# Retrieve Training Data for Poisson Model\n",
    "\n",
    "conn = pymssql.connect(server, user, password, database)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Use a Selct Query to get rows from SQL table\n",
    "cursor.execute(f\"SELECT * FROM {table}\")\n",
    "\n",
    "# Iterate over rows stored in cursor and place them in a list\n",
    "row_list = []\n",
    "for row in cursor:\n",
    "    row_list.append(row)\n",
    "\n",
    "# Convert list of rows into a Pandas DataFrame\n",
    "df = pd.DataFrame(row_list, columns = ['State','County','Population Estimate (2019)','Urban Category',\n",
    "'Median Household Income (2019)','Number of Unemployed People (2019)','Unemployment Rate (2019)',\n",
    "'Number of Uninsured (2019)','Latitude','Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to proper data types\n",
    "df[['Median Household Income (2019)','Number of Unemployed People (2019)']] = df[['Median Household Income (2019)','Number of Unemployed People (2019)']].astype(int)\n",
    "df['Unemployment Rate (2019)'] = df['Unemployment Rate (2019)'].astype(float)\n",
    "df['Population Estimate (2019)'] = df['Population Estimate (2019)'].astype(int)\n",
    "df['Urban Category'] = df['Urban Category'].astype(int)\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    "\n",
    "# Make new column with % of Uninsured\n",
    "df['% Uninsured (2019)'] = df['Number of Unemployed People (2019)']/df['Population Estimate (2019)'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a geometry column \n",
    "geometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\n",
    "\n",
    "# Coordinate reference system : WGS84\n",
    "crs = {'init': 'epsg:4326'}\n",
    "\n",
    "# Creating a Geographic data frame \n",
    "gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and a single axis\n",
    "f, ax = plt.subplots(1, figsize=(30, 5))\n",
    "\n",
    "# Build choropleth\n",
    "gdf.plot(\n",
    "    column='% Uninsured (2019)', \n",
    "    cmap='viridis', \n",
    "    scheme='natural_breaks',\n",
    "    k=5, \n",
    "    edgecolor='white', \n",
    "    linewidth=0., \n",
    "    alpha=0.75, \n",
    "    legend=True,\n",
    "    legend_kwds=dict(loc=2),\n",
    "    ax=ax\n",
    ")\n",
    "# Add basemap\n",
    "contextily.add_basemap(\n",
    "    ax, \n",
    "    crs=gdf.crs, \n",
    "    source=contextily.providers.CartoDB.VoyagerNoLabels,\n",
    ")\n",
    "# Remove axes\n",
    "\n",
    "\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = [\n",
    "    'Urban Category',\n",
    "    'Median Household Income (2019)',\n",
    "    'Unemployment Rate (2019)'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.model import spreg\n",
    "# Fit OLS model\n",
    "m1 = spreg.OLS(\n",
    "    # Dependent variable\n",
    "    gdf[['% Uninsured (2019)']].values, \n",
    "    # Independent variables\n",
    "    gdf[variable_names].values,\n",
    "    # Dependent variable name\n",
    "    name_y='% Uninsured', \n",
    "    # Independent variable name\n",
    "    name_x=variable_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m1.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Boolean (True/False) with whether a\n",
    "# property is urban or not\n",
    "is_urban = gdf['Urban Category'].astype(bool)\n",
    "# Split residuals (m1.u) between coastal and not\n",
    "urban = m1.u[is_urban]\n",
    "not_urban = m1.u[~is_urban]\n",
    "# Create histogram of the distribution of coastal residuals\n",
    "plt.hist(urban, density=True, label='Urban')\n",
    "# Create histogram of the distribution of non-coastal residuals\n",
    "plt.hist(\n",
    "    not_urban, \n",
    "    histtype='step',\n",
    "    density=True, \n",
    "    linewidth=4, \n",
    "    label='Rural'\n",
    ")\n",
    "# Add Line on 0\n",
    "plt.vlines(0,0,1, linestyle=\":\", color='k', linewidth=4)\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.title('Urban vs. Rural Error Distribution')\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 100\n",
    "seaborn.set(font_scale = 1.25)\n",
    "f, ax = plt.subplots(1,2,figsize=(12,3), sharex=True, sharey=True)\n",
    "ax[0].hist(\n",
    "    urban, \n",
    "    color=['r'], \n",
    "    alpha=1, \n",
    "    density=True, \n",
    "    bins=30, \n",
    "    label='Urban', \n",
    "    cumulative=True\n",
    ")\n",
    "ax[1].hist(\n",
    "    not_urban, \n",
    "    color=['b'], \n",
    "    alpha=1,\n",
    "    density=True, \n",
    "    bins=30, \n",
    "    label='Rural', \n",
    "    cumulative=True\n",
    ")\n",
    "for simulation in range(n_simulations):\n",
    "    shuffled_residuals = m1.u[np.random.permutation(m1.n)]\n",
    "    random_urban, random_urban = (\n",
    "        shuffled_residuals[is_urban], \n",
    "        shuffled_residuals[~is_urban]\n",
    "    )\n",
    "    ax[0].hist(\n",
    "        random_urban, \n",
    "        density=True, \n",
    "        histtype='step',\n",
    "        color='k', alpha=.05, bins=30, \n",
    "        label=None, \n",
    "        cumulative=True\n",
    "    )\n",
    "    ax[1].hist(\n",
    "        random_urban, \n",
    "        density=True, \n",
    "        histtype='step',\n",
    "        color='k', alpha=.05, bins=30, \n",
    "        label=None, \n",
    "        cumulative=True\n",
    "    )\n",
    "    \n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Empirical Cumulative Density Function')\n",
    "ax[1].set_title('Empirical Cumulative Density Function')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the observed residual distributions vs. random distributions for Urban and Rural Counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1160 Urban Counties vs. 1959 Rural Counties\n",
    "\n",
    "- The black lines represent our simulations, and the colored patches below them represents the observed distribution of residuals. \n",
    "\n",
    "- If the black lines tend to be on the **left** of the colored patch, then, the simulations (where prediction error is totally random with respect to our categories of \"Urban\" and \"Rural\") tend to have more **negative residuals** than our actual model **(over-predicting)**\n",
    "\n",
    "- If the black lines tend to be on the **right**, then they tend to have more **positive residuals** than our actual model **(under-predicting)**\n",
    "\n",
    "#### Our simulations provide direct evidence for the claim that our model may be systematically `over-predicting` Urban % Uninsured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = weights.KNN.from_dataframe(gdf, k=1)\n",
    "lag_residual = weights.spatial_lag.lag_spatial(knn, m1.u)\n",
    "ax = seaborn.regplot(\n",
    "    m1.u.flatten(), \n",
    "    lag_residual.flatten(), \n",
    "    line_kws=dict(color='orangered'),\n",
    "    ci=None\n",
    ")\n",
    "ax.set_xlabel('Model Residuals - $u$')\n",
    "ax.set_ylabel('Spatial Lag of Model Residuals - $W u$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our prediction errors tend to cluster! Above, we show the relationship between our prediction error at each site and the prediction error at the site nearest to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we’re using this nearest site to stand in for the surroundings of a County. This means that, when the model tends to **over-predict** a given County's % Uninsured, sites around that County are *ALSO* more likely to also be over-predicted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An interesting property of this relationship is that it tends to stabilize as the number of nearest neighbors used to construct each County's surroundings increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-weight W to 10 nearest neighbors\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15);\n",
    "knn.reweight(k=10, inplace=True)\n",
    "\n",
    "# Row standardise weights\n",
    "knn.transform = 'R'\n",
    "# Run (Local Spatial Autocorrelation) LISA on residuals\n",
    "outliers = esda.moran.Moran_Local(m1.u, knn, permutations=9999)\n",
    "\n",
    "# q values: 1 HH, 2 LH, 3 LL, 4 HL\n",
    "\n",
    "# Select only LISA cluster cores where the model overpredicts\n",
    "error_clusters = (outliers.q % 2 == 1)\n",
    "# Filter out non-significant clusters\n",
    "error_clusters &= (outliers.p_sim <= .001)\n",
    "# Add `error_clusters` and `local_I` columns\n",
    "ax = gdf.assign(\n",
    "    error_clusters = error_clusters,\n",
    "    local_I = outliers.Is\n",
    "# Retain error clusters only\n",
    ").query(\n",
    "    \"error_clusters\"\n",
    "# Sort by I value to largest plot on top\n",
    ").sort_values(\n",
    "    'local_I'\n",
    "# Plot I values\n",
    ").plot(\n",
    "    'local_I', cmap='bwr', marker='.'\n",
    ")\n",
    "# Add basemap\n",
    "contextily.add_basemap(ax, crs=gdf.crs)\n",
    "# Remove axes\n",
    "ax.set_axis_off();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Here, we are measuring spatial autocorrelation for our Counties using **Local Moran's I** Statistics. \n",
    "- #### **ESDA** is Exploratory Spatial Data Analysis\n",
    "- #### [What is Moran's I?](https://www.statisticshowto.com/morans-i/#:~:text=What%20is%20Moran's%20I%3F,the%20observations%20are%20not%20independent) Moran's I is a correlation coefficient that measures the overall spatial autocorrelation of your data set. In other words, it measures how one object is similar to others surrounding it. If objects are attracted (or repelled) by each other, it means that the observations are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### The highlighted areas tend to be locations where our model significantly **overpredicts** the % Uninsured both for that specific observation and observations in its immediate surroundings. \n",
    "- #### This is critical since, if we can identify how these areas are structured — in other words, if they have a *consistent* geography that we can model — then we might make our predictions even better, or at least not systematically mis-predict prices in some areas while correctly predicting prices in other areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = [\n",
    "    'Median Household Income (2019)',\n",
    "    'Unemployment Rate (2019)'\n",
    "]\n",
    "\n",
    "\n",
    "m5 = spreg.OLS_Regimes(\n",
    "    # Dependent variable\n",
    "    gdf[['% Uninsured (2019)']].values, \n",
    "    # Independent variables\n",
    "    gdf[variable_names].values,\n",
    "    # Variable specifying neighborhood membership\n",
    "    gdf['Urban Category'].tolist(),\n",
    "    # Allow the constant term to vary by group/regime\n",
    "    constant_regi='many',\n",
    "    # Allow separate sigma coefficients to be estimated\n",
    "    # by regime (False so a single sigma)\n",
    "    regime_err_sep=False,\n",
    "    # Dependent variable name\n",
    "    name_y='% Uninsured', \n",
    "    # Independent variables names\n",
    "    name_x=variable_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m5.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated Linear Regression Coefficients\n",
    "m5.betas"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f610223a794a5e00f0ba9c0cf804e6f0adda2cb0aeba7176f1b0ab3674a0abbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
